{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of worksheet: show that capacity of generator G is sufficient for estimating increasing complicated functions\n",
    "\n",
    "Generator to estimate function\n",
    "\n",
    "\n",
    "Goal $G(z)=f(z)$\n",
    "\n",
    "Where f is a known function and G is a the generator function that estimates f.\n",
    "\n",
    "$z\\in \\mathcal{N}_{n_z}(0,1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class basefunction():\n",
    "    \"\"\"Basic linear function, initialize once and keep A during experiment\"\"\"\n",
    "    def __init__(self,opt):\n",
    "        X_train,y_train,_,_ = load_labeled_data(\"data/exp1_standard5000.npz\")\n",
    "        X_train = X_train[y_train==0]\n",
    "\n",
    "        cov = np.cov(X_train[:,:opt.specsize],rowvar=False)\n",
    "        U, s, _ = np.linalg.svd(cov)\n",
    "        l = opt.latent_dim\n",
    "        \n",
    "        self.A = np.dot(U[:,:l],np.sqrt(np.diag(s[:l])))\n",
    "        self.bias = np.mean(X_train[:,:opt.specsize],axis=0)\n",
    "        # self.A = np.random.rand(opt.specsize,opt.latent_dim)\n",
    "        # self.A = np.ones((opt.specsize,opt.latent_dim))*0.1\n",
    "        # self.bias = np.ones(opt.specsize)*0.2\n",
    "    def forward(self,z):\n",
    "        x = (np.transpose(self.A@np.transpose(z))+self.bias) #*0.5\n",
    "        x += np.transpose(self.A@np.transpose(z**2)) #*0.5\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y=f(z)\\rightarrow \\mathbb{R}^n$\n",
    "\n",
    "$z=f^{-1}(y)$\n",
    "\n",
    "$y=f(f^{-1}(y))=G^*(z)$\n",
    "\n",
    "Where $G^*$ denotes the optimal G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class opt():\n",
    "    pass\n",
    "opt.batch_size = 1024\n",
    "opt.numclasses = 1\n",
    "opt.latent_dim = 100\n",
    "opt.specsize = 573\n",
    "opt.limitclasses = [0,2,6,7,8]\n",
    "fun = basefunction(opt)\n",
    "\n",
    "z = np.random.normal(0,1,(opt.batch_size,opt.latent_dim,))\n",
    "y = fun.forward(z)\n",
    "opt.specsize = np.shape(y)[1]\n",
    "\n",
    "#example function output\n",
    "# print(y[0,:])\n",
    "# plt.plot(y[0,:])\n",
    "# plt.title(\"Single realization\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X\\approx f(z)=Az+\\mu$\n",
    "\n",
    "The best A matrix is given by a lower rank approximation of the covariance matrix.\n",
    "\n",
    "To get a baseline we make a linear estimation of A using least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on original data\n",
    "X_train,y_train,X_test,y_test = load_labeled_data(\"data/exp1_standard5000.npz\")\n",
    "X_train = X_train[y_train==0]\n",
    "X_test = X_test[y_test==0]\n",
    "y_train = y_train[y_train==0]\n",
    "y_test = y_test[y_test==0]\n",
    "\n",
    "opt.batch_size=len(X_train)\n",
    "z = np.random.normal(0,1,(opt.batch_size,opt.latent_dim,))\n",
    "A_hat = np.linalg.inv(z.T.dot(z)).dot(z.T).dot(X_train).T\n",
    "#apply estimater\n",
    "y_hat = A_hat.dot(z.T) #reconstruct original data\n",
    "\n",
    "reg = sklearn.linear_model.LinearRegression().fit(z,X_train)\n",
    "\n",
    "\n",
    "lossfun = torch.nn.MSELoss()\n",
    "print(\"MSEloss unbiased lsq original \",lossfun(torch.tensor(X_train),torch.tensor(y_hat.T)).item())\n",
    "error = error_val(X_train,y_hat.T)\n",
    "print(\"FD lsq original\",error.FID())\n",
    "print(\"MSEloss linear regression original \",lossfun(torch.tensor(X_train),torch.tensor(reg.predict(z))).item())\n",
    "error = error_val(X_train,reg.predict(z))\n",
    "print(\"FD linear regression original\",error.FID())\n",
    "print(\"MSEloss train/test \",lossfun(torch.tensor(X_train[0:len(y_test)]),torch.tensor(X_test[0:len(y_train)])).item())\n",
    "error = error_val(X_train,X_test)\n",
    "print(\"FD train/test\",error.FID())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "opt.batchsize=1024\n",
    "z = np.random.normal(0,1,(opt.batch_size,opt.latent_dim,))\n",
    "y = fun.forward(z)\n",
    "\n",
    "# Linear estimater\n",
    "A_hat = np.linalg.inv(z.T.dot(z)).dot(z.T).dot(y).T\n",
    "print(np.shape(A_hat))\n",
    "print(np.shape(fun.A))\n",
    "#apply estimater\n",
    "y_hat = A_hat.dot(z.T)\n",
    "\n",
    "reg = sklearn.linear_model.LinearRegression().fit(z,y)\n",
    "lossfun = torch.nn.MSELoss()\n",
    "# print(reg.coef_)\n",
    "print(\"Number of parameters estimated correctly by regression\",np.sum(np.abs(fun.A-reg.coef_)<0.0001))\n",
    "print(\"Number of parameters estimated correctly by lsq\",np.sum(np.abs(fun.A-A_hat)<0.0001))\n",
    "\n",
    "print(\"MSEloss unbiased lsq\",lossfun(torch.tensor(y),torch.tensor(y_hat.T)).item())\n",
    "print(\"MSEloss linear regression\",lossfun(torch.tensor(y),torch.tensor(reg.predict(z))).item())\n",
    "error_reg = error_val(y,reg.predict(z))\n",
    "error_lsq = error_val(y,y_hat.T)\n",
    "print(\"Regression frechet distance: \",error_reg.FID())\n",
    "print(\"Linear estimate frechet distance: \",error_lsq.FID())\n",
    "\n",
    "plot_spec_stats(y,y_hat.T,range=[0,len(y.T)],compare_var=True)\n",
    "plot_spec_stats(y,reg.predict(z),range=[0,len(y.T)],compare_var=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal G(z) -> f(z)\n",
    "To do this we minimize error:\n",
    "\n",
    "$\\min_{\\theta_G}\\mathbf{E}||f(z)-G(z)||^2$\n",
    "\n",
    "G(z) is a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G = cGANgenerator_fc_skipconnect(opt) # initialize fully connected generator\n",
    "# G = simplest_generator(opt) #single layer\n",
    "# G = cGANgenerator_fc_minuslayer(opt)\n",
    "# G= cGANgenerator_fc_addlayer(opt)\n",
    "# G = cGANgenerator_fc(opt)\n",
    "G = cDC_Generator_fc(opt)\n",
    "\n",
    "\n",
    "\n",
    "labels = torch.LongTensor(np.zeros((opt.batch_size))) #labels not taken in to account -> all labels 0\n",
    "# print(G)\n",
    "lossfun = torch.nn.MSELoss()\n",
    "# optim = torch.optim.Adam(G.parameters(),lr=0.0001)\n",
    "optim = torch.optim.SGD(G.parameters(),lr=0.0005)\n",
    "losslist = []\n",
    "fidlist = []\n",
    "\n",
    "last_loss = 1000\n",
    "best_loss = 1000\n",
    "notincreasing_count = 0\n",
    "BestG=copy.deepcopy(G)\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    # sample inputnoise\n",
    "    z = np.random.normal(0,1,(opt.batch_size,opt.latent_dim,))\n",
    "    # create real samples\n",
    "    y = torch.FloatTensor(fun.forward(z))\n",
    "    # dreate fake samples\n",
    "    yhat = G(torch.FloatTensor(z),labels)\n",
    "    #calculate loss and update\n",
    "    loss = lossfun(yhat,y.squeeze(1))\n",
    "    error = error_val(yhat.detach().numpy(),y.squeeze(1).detach().numpy())\n",
    "    FD = Variable(torch.tensor(error.FID()),requires_grad=True)\n",
    "    # loss = FD\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    losslist.append(loss.item())\n",
    "    fidlist.append(FD.item())\n",
    "    if i%100==0:\n",
    "        yhat = BestG(torch.FloatTensor(z),labels)\n",
    "        if opt.specsize<9:\n",
    "            plt.figure()\n",
    "            plt.hist(yhat.detach().numpy()[:,0],bins=20,alpha=0.5)\n",
    "            plt.hist(y.detach().numpy()[:,0],bins=20,alpha=0.5)\n",
    "            plt.legend([\"G(z)\",\"f(z)\"])\n",
    "            plt.xlim(-1,1)\n",
    "            plt.savefig(str(\"test/feature_1_iter_\"+str(i)))\n",
    "        else:\n",
    "            plt = plot_spec_stats(y.detach().numpy(),yhat.detach().numpy(),compare_var=True,range=[0,opt.specsize])\n",
    "            # plt.ylim(-5,5)\n",
    "            plt.savefig(str(\"test/spec_stats_iter_\"+str(i)))\n",
    "\n",
    "    if loss.item()>best_loss:\n",
    "        notincreasing_count+=1\n",
    "    else:\n",
    "        BestG=copy.deepcopy(G)\n",
    "        notincreasing_count=0\n",
    "    last_loss = loss.item()\n",
    "    \n",
    "    if last_loss<best_loss: #remember best loss value\n",
    "        best_loss=last_loss\n",
    "    if notincreasing_count>100:\n",
    "        yhat = BestG(torch.FloatTensor(z),labels)\n",
    "        if opt.specsize<9:\n",
    "            plt.figure()\n",
    "            plt.hist(yhat.detach().numpy()[:,0],bins=20,alpha=0.5)\n",
    "            plt.hist(y.detach().numpy()[:,0],bins=20,alpha=0.5)\n",
    "            plt.legend([\"G(z)\",\"f(z)\"])\n",
    "            plt.xlim(-1,1)\n",
    "            plt.savefig(str(\"test/simplestgen/feature_1_iter_\"+str(i)))\n",
    "        else:\n",
    "            plt = plot_spec_stats(y.detach().numpy(),yhat.detach().numpy(),compare_var=True,range=[0,opt.specsize])\n",
    "            # plt.ylim(-5,5)\n",
    "            plt.savefig(str(\"test/simplestgen/spec_stats_iter_\"+str(i)))\n",
    "        break\n",
    "yhat = G(torch.FloatTensor(z),labels)\n",
    "plt = plot_spec_stats(y.detach().numpy(),yhat.detach().numpy(),compare_var=True,range=[0,opt.specsize])\n",
    "plt.savefig(\"test/simplestgen/spec_stats_iter_final\")\n",
    "\n",
    "print(\"Lowest achieved loss\",best_loss)\n",
    "print(\"Lowest FD distance: \",np.min(fidlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G)\n",
    "print(G.__dict__)\n",
    "# print(G.model[0].weight)\n",
    "# print(fun.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losslist)\n",
    "plt.title('loss over time')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('MSE loss')\n",
    "\n",
    "# print(yhat.detach().numpy()[0])\n",
    "plt.figure()\n",
    "plt.plot(yhat.detach().numpy()[0])\n",
    "plt.plot(yhat.detach().numpy()[1])\n",
    "plt.title(\"2 generated realizations\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fidlist)\n",
    "plt.ylim(0)\n",
    "plt.title(\"FD over time\")\n",
    "# plt.ylim([0,100])\n",
    "print(fidlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # validation\n",
    "G.eval()\n",
    "print(G)\n",
    "# print(G.model[0].weight)\n",
    "# labels = torch.LongTensor(np.zeros((1))) #labels not taken in to account -> all labels 0\n",
    "\n",
    "# labels = torch.LongTensor(np.zeros((100000)))\n",
    "z = np.random.normal(0,1,(100000,opt.latent_dim,))\n",
    "# # create real samples\n",
    "y = torch.FloatTensor(fun.forward(z)).squeeze(1).detach().numpy()\n",
    "yhat = G(torch.FloatTensor(z),labels).detach().numpy()\n",
    "\n",
    "# xhat = scaler.inverse_transform(yhat) # for FD calculation\n",
    "# yfz = scaler.inverse_transform(y) # for FD calculation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distributions\n",
    "plt.figure()\n",
    "plt.hist(yhat[:,0],bins=20,alpha=0.5)\n",
    "plt.hist(y[:],bins=20,alpha=0.5)\n",
    "plt.legend([\"G(z)\",\"f(z)\"])\n",
    "plt.show()\n",
    "\n",
    "print(np.shape(y))\n",
    "print(np.shape(yhat))\n",
    "print(np.mean(np.std(y)))\n",
    "# plot_spec_stats(y,yhat,compare_var=True,range=[0,len(yhat[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(np.corrcoef(y,rowvar=False))\n",
    "plt.title('Original')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.corrcoef(yhat,rowvar=False))\n",
    "plt.title('Fake')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COV = np.random.uniform(0,1,size=(2,2))\n",
    "# COV = (COV + COV.T)/2\n",
    "# # print(COV)\n",
    "# U, S, _ = np.linalg.svd(COV)\n",
    "\n",
    "# l=2\n",
    "# print(S)\n",
    "# print(np.sqrt(np.diag(S[:l])*np.sqrt(np.diag(S[:l]))\n",
    "# A = U[:,:l] @ np.sqrt(np.diag(S[:l]))\n",
    "# print(COV)\n",
    "# print(A@A.T)\n",
    "# print(np.linalg.norm(COV-A@A.T))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
